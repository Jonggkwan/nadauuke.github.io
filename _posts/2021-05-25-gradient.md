---
title:  "3. 그레디엔트 소실 및 폭주"

categories:
  - Deep Learning
tags:
  - Deep Learning
use_math: True
    
---


### 3. 그레디엔트 소실 및 폭주
\
역전파 알고리즘은 출력층에서 입력층으로 오차 radient를 전파하면서 진행됩니다.
알고리즘이 각각 layer에서 모든 파라미터에 대한 오차 함수의 radient를 계산하면 경사 하강법 단계에서 이 gradient를
사용하여 각 파라미터를 수정합니다.
\

그러나 학습시 알고리즘이 하위층으로 진행될수록 gradient가 점점 작아지는 경우가 많습니다. 경사 하강법이 하위층의 연결 가중치를
변경되지 않은 채로 둔다면 훈련이 좋은 솔루션으로 수렴되지 않습니다. 이러한 문제를 vanishing gradient(그레디언트 소실)이라고 합니다.
어떤 경우에는 반대 현상이 일어날 수 있습니다. gradient가 점점 커져서 여러층이 큰 가중치로 갱신되면 알고리즘이 발산합니다.
이러한 문제를 exploding gradient(그레디언트 폭즈)라고 하며 순환 신경망에서 주로 이러납니다.

