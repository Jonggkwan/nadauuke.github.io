---
title:  "3. 그레디엔트 소실 및 폭주"

categories:
  - Deep Learning
tags:
  - Deep Learning
use_math: True
    
---


### 3. 그레디엔트 소실 및 폭주
\
역전파 알고리즘은 출력층에서 입력층으로 오차 radient를 전파하면서 진행됩니다.
알고리즘이 각각 layer에서 모든 파라미터에 대한 오차 함수의 radient를 계산하면 경사 하강법 단계에서 이 gradient를
사용하여 각 파라미터를 수정합니다.
\

그러나 학습시 알고리즘이 하위층으로 진행될수록 gradient가 점점 작아지는 경우가 많습니다. 경사 하강법이 하위층의 연결 가중치를
변경되지 않은 채로 둔다면 훈련이 좋은 솔루션으로 수렴되지 않습니다. 이러한 문제를 vanishing gradient(그레디언트 소실)이라고 합니다.
어떤 경우에는 반대 현상이 일어날 수 있습니다. gradient가 점점 커져서 여러층이 큰 가중치로 갱신되면 알고리즘이 발산합니다.
이러한 문제를 exploding gradient(그레디언트 폭즈)라고 하며 순환 신경망에서 주로 이러납니다.
\

일반적으로 불안정한 그레디언트는 훈련을 어렵게합니다. 층마다 학습속도가 달라지기 떄문입니다.
이러한 현상은 주로 activation function을 logistic sigmoid function과 
가중치 초기화 방법(평균이 0이고 표준편차가 1인 정규분포로 변환)을 조합하는 경우에 주로 발생했습니다.
신경망의 위쪽으로 갈수록 층을 지날 때마다 분산이 계족 커져 가장 높은 층에는 활성화 함수가 0이나 1로 수렴합니다.
이는 로지스틱 시그모이드 함수가 평균이 0.5라는 사실 때문에 더 나빠집니다.
