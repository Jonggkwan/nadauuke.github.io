---
title:  "인공 신경망 개요"

categories:
  - Deep Learning
tags:
  - Deep Learning
use_math: True
    
---



### 1. Perceptron

가장 간단한 인공신경망중 하나로 TLU(Threshold Logic unit) 또는 LTU(Linear Threshold Unit)이라고 불리는
다른 형태의 인공 뉴런을 기반으로 합니다.

입력과 출력이 어떤 숫자이고 각각의 연결은 가중치(weight)와 연결되어 있습니다.

TLU는 입력의 가중치 합을 계산한 뒤 Step function 을 적용해 결과를 출력합니다.

$h_w(x)\ =\ step(z),\ \ z\ =\ x^Tw$

퍼셉트론에서 많이 쓰이는 step function은 Heaviside step function입니다.
혹은 sign function을 쓰기도 합니다.

$heaviside(z)=\left\lbrace\begin{matrix}\ 0 \ (z < 0 ) \\\ 1 \ (z\geq 0)\end{matrix}\right. \, \  \ \ sgn(z)=\left\lbrace\begin{matrix}\ -1 \  (z < 0 ) \\\ 0 \ \ \ (z = 0)\\\ 1\ \ \ (z>0)\end{matrix}\right.$

하나의 TLU는 간단한 선형 이진 분류에 사용할 수 있습니다. 
입력의 선형 조합을 계산해서 그 결과가 입계값을 넘으면 양성을 출력하고 그렇지 않으면 음성을 출력합니다. 
또한 TLU를 훈련 시킨다는 것은 최적의 weight을 찾는 다는 뜻입니다.

퍼셉트론은 층이 하나뿐인 TLU로 구성됩니다. 각각의 TLU는 모든 입력에 연결되어 있습니다.
한층에 있는 모든 뉴런이 이전층의 모든 뉴런과 연결되어 있을 때 이를 dense layer(밀집층) 혹은 
완전 연결 층(fully connected layer)이라고 부릅니다.

퍼셉트론의 입력은 입력뉴런이라는 통과 뉴런에 주입됩니다. 입력층은 모두 입력뉴런으로 구성되고 보통 편향(bias)가 더해집니다.

완전연결층의 출력 계산은
$ h_{w,b} = \phi (XW +b) $ 입니다.

X는 입력특성의 행렬이고 W는 bias를 제외한 weight 입니다.
$\phi$는 활성화 함수(activation function) 입니다.  TLU의 경우 이 함수는 step function 입니다.
