---
title:  "인공 신경망 개요"

categories:
  - Deep Learning
tags:
  - Deep Learning
use_math: True
    
---



### 1. Perceptron ###
\
\
가장 간단한 인공신경망중 하나로 TLU(Threshold Logic unit) 또는 LTU(Linear Threshold Unit)이라고 불리는
다른 형태의 인공 뉴런을 기반으로 합니다.

입력과 출력이 어떤 숫자이고 각각의 연결은 가중치(weight)와 연결되어 있습니다.

TLU는 입력의 가중치 합을 계산한 뒤 Step function 을 적용해 결과를 출력합니다.

$$ h_w(x)\ =\ step(z),\ \ z\ =\ x^Tw $$

퍼셉트론에서 많이 쓰이는 step function은 Heaviside step function입니다.
혹은 sign function을 쓰기도 합니다.

$$ heaviside(z)=\left\lbrace\begin{matrix}\ 0 \ (z < 0 ) \\\ 1 \ (z\geq 0)\end{matrix}\right. $$
$$ sgn(z)=\left\lbrace\begin{matrix}\ -1 \  (z < 0 ) \\\ 0 \ \ \ (z = 0)\\\ 1\ \ \ (z>0)\end{matrix}\right. $$

하나의 TLU는 간단한 선형 이진 분류에 사용할 수 있습니다. 
입력의 선형 조합을 계산해서 그 결과가 입계값을 넘으면 양성을 출력하고 그렇지 않으면 음성을 출력합니다. 
또한 TLU를 훈련 시킨다는 것은 최적의 weight을 찾는 다는 뜻입니다.

퍼셉트론은 층이 하나뿐인 TLU로 구성됩니다. 각각의 TLU는 모든 입력에 연결되어 있습니다.
한층에 있는 모든 뉴런이 이전층의 모든 뉴런과 연결되어 있을 때 이를 dense layer(밀집층) 혹은 
완전 연결 층(fully connected layer)이라고 부릅니다.

퍼셉트론의 입력은 입력뉴런이라는 통과 뉴런에 주입됩니다. 입력층은 모두 입력뉴런으로 구성되고 보통 편향(bias)가 더해집니다.

완전연결층의 출력 계산은
$$ h_{w,b} = \phi (XW +b) $$ 입니다.

X는 입력특성의 행렬이고 W는 bias를 제외한 weight 입니다.
$$ \phi $$는 활성화 함수(activation function) 입니다.  TLU의 경우 이 함수는 step function 입니다.


퍼셉트론은 네트워크가 예측할 때 만드는 오차가 생기면 오차를 줄이는 방향으로 weight을 강화시킵니다.

$$ w_{i,j}^{(next step)} = w_{i,j} + n (y_j- \hat{y_j})x_i $$

$$w_{i,j} $$는 i번째 뉴런과 j번째 뉴런 사이를 연결하는 가중치 입니다. \
$$ x_i $$ 는 현재 훈련 샘플의 i번째 뉴런의 입력값입니다.\
$$ \hat{y_j} $$은 훈련 샘플의 j번째 출력입니다.
$$ y_j $$ 는 훈련샘플의 j번쨰 타깃값입니다. \
$$ n $$은 학습률입니다.\


각 출력 뉴런의 결정경계는 선형이므로 퍼셉트론도 복잡한 패턴을 학습하지 못합니다.\
대신 훈련샘플이 선형적으로 구분될 수 있으면 정답에 수렴합니다.\
이 제약을 Multi Layer Perceptron (다층 퍼셉트론)을 통해 해결 할 수 있습니다.\


### 2.DNN과 Backpropagation
\
다층 퍼셉트론은 하나의 입력층과 하나의 출력층 그리고 그 사이에 은닉층이라는 하나 이상의 TLU층으로 구성됩니다.\
DNN을 훈련하는 알고리즘을 역전파(Backpropagation)이라 합니다. \
이 알고리즘은 간단히 말하면 효율적으로 gradient를 계산하는 경사하강법입니다.\
네트워크를 두번(정방향, 역방향) 통과하는 것만으로 모든  모델 파라미터에 대한 네트워크의 오차의 gradient를 계산할 수 있습니다.\
Gradient를 구하고 평범한 경사 하강법을 수행합니다.
\
알고리즘을 간단히 설명하면 


